{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d91286-3d14-4fa1-9ee5-191eb75b4964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime \n",
    "\n",
    "# Pre-Processing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modelling\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GRU, Conv1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Fine-Tuning and Tracking\n",
    "import wandb\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, matthews_corrcoef\n",
    "\n",
    "# Hide Debug Info\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303bef0-d59f-4afb-a357-b1107f27e314",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../Data/Processed/processed_binary.json')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6c838-595c-46c5-905d-efd36a40df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Weights and Bias\n",
    "wandb.init(project=\"misinformation_nlp_BERT\")\n",
    "\n",
    "config = dict(\n",
    "    model='RoBERTaO_LSTM',\n",
    "    num_words = 25000,\n",
    "    learning_rate = 0.0001,\n",
    "    units = 128,\n",
    "    input_len = 70,\n",
    "    dropout = 0.5,\n",
    "    batch_size = 256,\n",
    "    epochs = 3,\n",
    "    trainable = True,\n",
    "    shuffle = True,\n",
    ")\n",
    "wandb.config.update(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce6606b-0ba8-404a-9f7f-a85ba911b2a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertEmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_name='distilroberta-base', trainable=wandb.config.trainable, **kwargs):\n",
    "        super(BertEmbeddingLayer, self).__init__(**kwargs)\n",
    "        self.bert = TFRobertaModel.from_pretrained(model_name, trainable=trainable)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Using the sequence output, not the pooled output\n",
    "        return self.bert(inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426f935-490b-4986-87a4-6662ddcf3417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import TFRobertaModel\n",
    "\n",
    "# Define input layers\n",
    "input_ids_layer = Input(shape=(wandb.config.input_len,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask_layer = Input(shape=(wandb.config.input_len,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "roberta_model = TFRobertaModel.from_pretrained('distilroberta-base')\n",
    "\n",
    "bert_output = BertEmbeddingLayer()([input_ids_layer, attention_mask_layer])\n",
    "lstm_output = LSTM(units=wandb.config.units)(bert_output)\n",
    "dropout_output = Dropout(wandb.config.dropout)(lstm_output)\n",
    "output_layer = Dense(1, activation='sigmoid')(dropout_output)\n",
    "\n",
    "model = Model(inputs=[input_ids_layer, attention_mask_layer], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=wandb.config.learning_rate), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42797be4-55d1-4e76-853f-690b04b745ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=wandb.config.input_len, return_tensors=\"tf\")\n",
    "\n",
    "tokenized_texts = tokenize_function(df['tweet'].tolist())\n",
    "\n",
    "# Extract input_ids and attention_masks from tokenized_texts\n",
    "input_ids_np = tokenized_texts['input_ids'].numpy()\n",
    "attention_masks_np = tokenized_texts['attention_mask'].numpy()\n",
    "targets_np = df['target_binary'].to_numpy()\n",
    "\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "    input_ids_np, targets_np, test_size=0.25, random_state=14, stratify=targets_np\n",
    ")\n",
    "train_masks_np, test_masks_np, train_, test_ = train_test_split(\n",
    "    attention_masks_np, targets_np, test_size=0.25, random_state=14, stratify=targets_np\n",
    ")\n",
    "\n",
    "# Convert the numpy arrays back to tensors if needed\n",
    "X_train = tf.convert_to_tensor(X_train_np)\n",
    "X_test = tf.convert_to_tensor(X_test_np)\n",
    "y_train = tf.convert_to_tensor(y_train_np)\n",
    "y_test = tf.convert_to_tensor(y_test_np)\n",
    "train_masks = tf.convert_to_tensor(train_masks_np)\n",
    "test_masks = tf.convert_to_tensor(test_masks_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40b0f4-82ea-4138-a2d9-a3cad15c8003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  \n",
    "    patience=2,           \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "history = model.fit({'input_ids': X_train, 'attention_mask': train_masks}, y_train, batch_size=wandb.config.batch_size, epochs=wandb.config.epochs, validation_split=0.15, shuffle=wandb.config.shuffle, callbacks=[early_stopping, wandb.keras.WandbCallback()])\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "training_time = (end_time - start_time).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067795a-218d-4160-8e53-ddd9aeb49882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get model predictions for both training and test sets\n",
    "predictions_train = model.predict([X_train, train_masks])\n",
    "predictions = model.predict([X_test, test_masks])\n",
    "\n",
    "train_preds_binary = (predictions_train > 0.5).astype(int)\n",
    "test_preds_binary = (predictions > 0.5).astype(int)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, train_preds_binary)\n",
    "test_accuracy = accuracy_score(y_test, test_preds_binary)\n",
    "\n",
    "# Print out the accuracies\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e1e29-4df3-4fb0-b25d-ae53308fc1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_test, predictions)\n",
    "mcc = matthews_corrcoef(y_test, test_preds_binary)\n",
    "\n",
    "print(\"AUC:\", auc)\n",
    "print(\"MCC:\", mcc)\n",
    "print(\" --- CLASSIFICATION REPORT --- \" )\n",
    "print(classification_report(y_test, test_preds_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8001904-adfd-4f4e-bafb-f00043d86d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_val_df = pd.read_json('../Data/Processed/processed_binary_val.json')\n",
    "external_val_df = pd.read_json('../Data/Cross_Validation/COVID_processed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c7442d-6c0b-4ba2-83b2-02f95491acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_tokenized_texts = tokenize_function(internal_val_df['tweet'].tolist())\n",
    "e_tokenized_texts = tokenize_function(external_val_df['tweet'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc2a8c-5921-45a1-acd7-3e7ef5269cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i_predictions = model.predict({'input_ids': i_tokenized_texts['input_ids'], 'attention_mask': i_tokenized_texts['attention_mask']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6fd2a-bb25-418e-92a7-130de0963998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "e_predictions = model.predict({'input_ids': e_tokenized_texts['input_ids'], 'attention_mask': e_tokenized_texts['attention_mask']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c02f71-b3bc-436b-ba70-c756737fb701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i_predictions_binary = (i_predictions > 0.5).astype(int)\n",
    "e_predictions_binary = (e_predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcabad00-d5d6-4a51-a56c-d6cd2d4ab8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_val_mcc = matthews_corrcoef(internal_val_df['target_binary'], i_predictions_binary)\n",
    "i_val_acc = accuracy_score(internal_val_df['target_binary'], i_predictions_binary)\n",
    "print(f\"(I) Validation: Matthews Correlation Coefficient: {i_val_mcc}\")\n",
    "print(f\"(I) Validation: Accuracy: {i_val_acc}\")\n",
    "print(\"---\")\n",
    "e_val_mcc = matthews_corrcoef(external_val_df['target'], e_predictions_binary)\n",
    "e_val_acc = accuracy_score(external_val_df['target'], e_predictions_binary)\n",
    "print(f\"(I) Validation: Matthews Correlation Coefficient: {e_val_mcc}\")\n",
    "print(f\"(E) Validation Accuracy: {e_val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ffd1c-3831-4acb-bfef-f5fbc020db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_val_mcc = i_val_mcc + e_val_mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fc3e49-0f2b-401f-869f-a8ff0c9cd95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({'Train Accuracy': train_accuracy, 'Test Accuracy': test_accuracy, 'AUC': auc, 'MCC': mcc, 'Training Time': training_time, '(Internal) Validation MCC': i_val_mcc, '(Internal) Validation ACC': i_val_acc, '(External) Validation MCC': e_val_mcc, '(External) Validation Accuracy': e_val_acc, 'Total Validation MCC': total_val_mcc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5cc47d-b20b-404b-805c-5097e91c50a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b20a2e-25e1-4fa8-9a94-b0122372e9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Assuming `predictions` are your model's output probabilities for the positive class\n",
    "fig = px.histogram(predictions, nbins=50, labels={'value': 'Prediction Confidence'})\n",
    "fig.update_layout(title='Distribution of Prediction Confidence')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc94f6-e6f3-483f-b840-02906b64c8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
